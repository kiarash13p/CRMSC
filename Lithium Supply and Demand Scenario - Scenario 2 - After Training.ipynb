{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2 - New trained model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repositories\\CRMSC\\LlamaChatTraining Environment\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`AnnotionFormat` is deprecated and will be removed in v4.38. Please use `transformers.image_utils.AnnotationFormat` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\GitHub repositories\\CRMSC\\LlamaChatTraining Environment\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "bin d:\\GitHub repositories\\CRMSC\\LlamaChatTraining Environment\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repositories\\CRMSC\\LlamaChatTraining Environment\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from typing import List, Optional\n",
    "import llama\n",
    "from llama import Llama, Dialog\n",
    "# import datasets # needed for handling datasets\n",
    "from datasets import (  load_dataset_builder, # finding info, description, etc.\n",
    "                        load_dataset, # Loading from our Huggingface profile\n",
    "                        )\n",
    "# import transformers\n",
    "from transformers import (\n",
    "    LlamaForCausalLM , \n",
    "    # LlamaTokenizer # Two core modules for handling model and tokenizer\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    IntervalStrategy,\n",
    "    pipeline,\n",
    "    logging\n",
    ")\n",
    "# trl stands for Transformer Reinforcement Learning\n",
    "from trl import SFTTrainer\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define major elements of Llama2 7b\n",
    "os.environ['models_loc'] : str                  =       'D:\\GitHub repositories\\llama\\\\'\n",
    "os.environ['ckpt_dir']   : str                  =       os.environ['models_loc'] + 'llama-2-7b-chat' \n",
    "os.environ['tokenizer_path'] : str              =       os.environ['models_loc'] + 'tokenizer,model'\n",
    "os.environ['ckpt_dir_crmsc']   : str            =       os.environ['models_loc'] + 'llama-2-7b-chat-hf' \n",
    "os.environ['ckpt_dir_crmsc_output']   : str     =       os.environ['models_loc'] + 'llama-2-7b-chat-hf-crmsc' \n",
    "os.environ['RANK']                      =       '0'\n",
    "os.environ['WORLD_SIZE']                =       '1'\n",
    "os.environ['MASTER_ADDR']               =       'localhost'\n",
    "os.environ['MASTER_PORT']               =       '12355'\n",
    "B_INST, E_INST              =   \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS                =   \"<<SYS>>\", \"<</SYS>>\"\n",
    "PAD                         =   '[PAD]'\n",
    "train_dataset               =       [\n",
    "    'env_1 - converted.txt',\n",
    "    # 'env_2 - converted.txt',\n",
    "    # 'env_3 - converted.txt',\n",
    "    # 'env_4 - converted.txt',\n",
    "    ]\n",
    "validation_dataset          =   [\n",
    "    'eenv_1 - converted.txt',\n",
    "]\n",
    "bnb_4bit_compute_dtype                  =       'float16' # Compute dtype for 4-bit base models\n",
    "use_4bit                                =       True # Activate 4-bit precision base model loading\n",
    "bnb_4bit_quant_type                     =       'nf4' # Quantization type (fp4 or nf4)\n",
    "use_nested_quant                        =       False # Activate nested quantization for 4-bit base models\n",
    "__cuda                                  =       torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "device_map                              =       __cuda#{\"\": 0 } # Load the entire \n",
    "lora_alpha                              =       64 \n",
    "lora_dropout                            =       0.05\n",
    "lora_r                                  =       512 # might be too much, needs to be modified later\n",
    "per_device_train_batch_size     =   2\n",
    "per_device_eval_batch_size      =   2\n",
    "gradient_accumulation_steps     =   1       #  Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "gradient_checkpointing          =   False   # Default is false,  If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "optim                           =   \"paged_adamw_32bit\" # adamw_torch , adamw_hf\n",
    "save_steps                      =   100  # save every x steps\n",
    "logging_steps                   =   1   # log every x updates steps\n",
    "learning_rate                   =   8e-4\n",
    "fp16                            =   False   #   Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
    "bf16                            =   False   #   Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training\n",
    "max_grad_norm                   =   0.64     #   Maximum gradient norm (for gradient clipping). default is 1.0\n",
    "max_steps                       =   -1      #   number of optimizer update steps / training steps to perform\n",
    "# warmup_ratio                    =   0     #   Ratio of total training steps used for a linear warmup from 0 to learning_rate.\n",
    "# warmup_steps                    =   0\n",
    "# weight_decay                    =   0.0\n",
    "# group_by_length                 =   True\n",
    "# lr_scheduler_type               =   \"linear\"    # better than cosine\n",
    "max_seq_length                  =   None\n",
    "max_new_tokens                  =   600\n",
    "packing                         =   False # use packing dataset training\n",
    "evalaution_strategy             =   IntervalStrategy.STEPS\n",
    "# settings for tokenizer\n",
    "padding_side                    =   'left'\n",
    "max_length                      =   400 # this might be as same as max_seq_length, but for making a difference between trainer and tokenizer, we defined this parameter\n",
    "clean_up_tokenization_spaces    =   True # False by default\n",
    "use_default_system_prompt       =   True # False by default\n",
    "# Inhertir from Guardrail ML ( https://colab.research.google.com/drive/134o_cXcMe_lsvl15ZE_4Y75Kstepsntu?usp=sharing#scrollTo=nAMzy_0FtaUZ )\n",
    "def load( padding_side : str = padding_side):\n",
    "    compute_dtype   =   getattr(torch,bnb_4bit_compute_dtype) # focusing on 4 bits quantization\n",
    "    bnb_config      =   BitsAndBytesConfig (\n",
    "        load_in_4bit    =   use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant\n",
    "    )\n",
    "    \n",
    "    # Initializing the model\n",
    "    # model   =   AutoModelForCausalLM.from_pretrained( os.environ['ckpt_dir_crmsc'],\n",
    "    #                                                 device_map              =   __cuda,\n",
    "    #                                                 quantization_config     =   bnb_config,\n",
    "    #                                                   )\n",
    "    # model.config.use_cache      =   False   # Whether or not the model should return the last key/values attentions\n",
    "    # model.config.pretraining_tp =   1       # for faster computation, but inaccurate, increase for better accuracy but slow calculation\n",
    "\n",
    "    # Initializing Parameter-Efficient Fine-Tuning configuration (Peft)\n",
    "    # Harnessing Low-Rank approximation technique\n",
    "    # peft_config     =   LoraConfig    (\n",
    "    #     lora_alpha=lora_alpha,\n",
    "    #     lora_dropout=lora_dropout,\n",
    "    #     r=lora_r,\n",
    "    #     bias='lora_only',\n",
    "    #     task_type=TaskType.CAUSAL_LM\n",
    "    # )\n",
    "\n",
    "    # Finally, loading tokenizer\n",
    "    # we use models location instead of '.model' to avoid warning, as in new version (v5) will be deprecated, also trust argument needs to be checked later\n",
    "    tokenizer       =   AutoTokenizer.from_pretrained( os.environ['ckpt_dir_crmsc'] , \n",
    "                                                      trust_remote_code=True,\n",
    "                                                      padding_side=padding_side,\n",
    "                                                      add_bos_token=False,   # bos is True by default\n",
    "                                                      add_eos_token=False,   # eos is False by default\n",
    "                                                      clean_up_tokenization_spaces  =   clean_up_tokenization_spaces, \n",
    "                                                      use_default_system_prompt     =   use_default_system_prompt,\n",
    "                                                      ) \n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # or 'tokenizer.eos_token\n",
    "    tokenizer.pad_token = E_INST\n",
    "    return tokenizer #, peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.79s/it]"
     ]
    }
   ],
   "source": [
    "peftmodel       =   LlamaForCausalLM.from_pretrained( os.environ['ckpt_dir_crmsc_output'] )\n",
    "# peftmodel       =   PeftModel.from_pretrained( model , os.environ['ckpt_dir_crmsc_output'] )\n",
    "# peftmodel.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scenario1    =   \"\"\"\n",
    "On this environment, we have one country called 'Z', in which one supplier and one consumer can be found, and their names are 'X' and 'A' respectively.\n",
    "There are no more countries on this environment\n",
    "This country holds 205 metric tons of Lithium Ore, providing it for its suppliers. suppliers do not supply ore but the hydroxide type of lithium to consumers.\n",
    "The supplier can convert Lithium Ore to Lithium Hydroxide by the conversion ratio of 0.9, meaning that one ton of Lithium Ore can be converted to 0.9 tons of Lithium Hydroxide.\n",
    "The supplier ('X') provides 4 tons of Lithium Hydroxide to the consumer, 'A'.\n",
    "The transportation delivery from 'X' to 'A' is 3 days.\n",
    "\"\"\"\n",
    "Question1       =   \"What are the relationships between suppliers and consumers? On which country are they located?\"\n",
    "Question2       =   \"What is the HHI of Lithium Ore on this environment?\"\n",
    "Question3       =   \"How much Lithium Hydroxide can be produced from one ton of lithium ore, considering the extraction and refining process?\"\n",
    "Question4       =   \"Can you make a brief report of the interactions between suppliers and consumers?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ask(  Question    :   str , Scenario   : str  = None ):\n",
    "    # New approach\n",
    "\n",
    "    generation_config   =   GenerationConfig(\n",
    "    num_beams       =   12, # by specifying a number of beams higher than 1, you are effectively switching from greedy search to beam search. This strategy evaluates several hypotheses at each time step and eventually chooses the hypothesis that has the overall highest probability for the entire sequence.\n",
    "    # early_stopping  =   True # No idea what it is\n",
    "    # decorder_start_token_id     =   0,\n",
    "    num_beams_group             =   3,\n",
    "    diversity_penalty           =   0.3,\n",
    "    do_sample                   =   False, # triggering group beam search\n",
    "    # top_k                       =   150,\n",
    "    top_p                       =   1.0,\n",
    "    temperature                 =   0.7,\n",
    "    # max_new_tokens              =   1024,\n",
    "    max_length                  =   2048,\n",
    "    early_stopping              =   True,\n",
    "    use_cache                   =   False,\n",
    "\n",
    "    eos_token_id                =   tokenizer.eos_token_id,   # End of sequence token\n",
    "    bos_token_id                =   tokenizer.bos_token_id,   # Beginning of sequence token\n",
    "    pad_token_id                =   tokenizer.pad_token_id    # padding token\n",
    ")\n",
    "    pipe        =   pipeline( \n",
    "    task='text-generation',\n",
    "    model=peftmodel,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    config=generation_config\n",
    ")\n",
    "    ## This is going to be deprecated\n",
    "    # generator   =   pipeline( task=\"text-generation\" , model=trainer.model , tokenizer=trainer.tokenizer )\n",
    "    # result      =   generator(f\"<s>[INST] {prompt} [/INST]\")\n",
    "    # print( result[0]['generated_text'])\n",
    "    Prompt          =   f\"<s>{B_SYS} {Scenario} {E_SYS}{B_INST} {Question} {E_INST}\"\n",
    "    Results         =   pipe( Prompt )\n",
    "    generated_text  =   Results[0]['generated_text']\n",
    "    index_end       =   generated_text.find( E_INST )\n",
    "    if index_end != -1: # something has been found :D\n",
    "        substring       =   generated_text[ index_end + len(E_INST) : ].strip()\n",
    "    else: # nothing is generated :(\n",
    "        substring       =   generated_text.strip()\n",
    "    print( \"*\" * 20 , end=' ' )\n",
    "    print( \" << Question >> \" , end=' ')\n",
    "    print( \"*\" * 20  )\n",
    "    print( Question )\n",
    "    print( \"*\" * 20 , end=' ' )\n",
    "    print( \" << Response >> \" , end=' ')\n",
    "    print( \"*\" * 20  )\n",
    "    print( substring )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ask( Question1 , Scenario1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ask( Question2 , Scenario1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ask( Question3 , Scenario1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ask( Question4 , Scenario1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad(): # Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True\n",
    "    # inputs = {k: v.to('cpu') for k, v in inputs.items()}\n",
    "    # outputs = model.generate( input_ids = inputs['input_ids'] , max_new_tokens = 100 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
