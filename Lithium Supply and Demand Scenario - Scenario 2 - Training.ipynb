{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2 - With some improvements\n",
    "\n",
    "As we witnessed some good and bad points on the previous attempt, the model `llama 2` require some training ( pre-training ) in order to provide suitable answers regarding supply chain of CRM. Thus, our idea here is to feed the model with some assumptions and Q&A, let it understand how to extract information and redesign responses more accurate. Our main assumption is that no external mathematical model is needed for purposes rather than text generation like modelling, estimating, or solving simple algebra equations.\n",
    "On this scenario, we are assuming that this model is able to predict the price of Lithium Hydroxide CIF well, given the price data from 15th of September till last day of October. We want to see the model's reaction and capability towards this request. \n",
    "\n",
    "We know that for forecasting the price, several model, most of which are based on AI, could execute this purpose accurately. Here, we want to see how close the `prediction` is to the reality.\n",
    "-   Will the model understand that no price is going to be published during weekends?\n",
    "-   Will the model realize the fact that prediction requires additional data, other than of numerical? For instance, will model request to be fed about geopolitical, vendornames, or other parameters which contribute to making disruptions in the price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from typing import List, Optional\n",
    "import llama\n",
    "from llama import Llama, Dialog\n",
    "# import datasets # needed for handling datasets\n",
    "from datasets import (  load_dataset_builder, # finding info, description, etc.\n",
    "                        load_dataset, # Loading from our Huggingface profile\n",
    "                        )\n",
    "# import transformers\n",
    "from transformers import (\n",
    "    # LlamaForCausalLM , \n",
    "    LlamaTokenizer, # Two core modules for handling model and tokenizer\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    get_scheduler,\n",
    "    SchedulerType,\n",
    "    AdamW,\n",
    "    training_args,\n",
    "    TrainingArguments,\n",
    "    IntervalStrategy,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    ")\n",
    "# trl stands for Transformer Reinforcement Learning\n",
    "from trl import SFTTrainer\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "import evaluate\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token 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 --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( torch.cuda.get_device_properties(device=0) )\n",
    "#torch.cuda.set_per_process_memory_fraction( 0.9 , 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define major elements of Llama2 7b\n",
    "os.environ['models_loc'] : str                  =       'D:\\GitHub repositories\\llama\\\\'\n",
    "os.environ['ckpt_dir']   : str                  =       os.environ['models_loc'] + 'llama-2-7b-chat' \n",
    "os.environ['tokenizer_path'] : str              =       os.environ['models_loc'] + 'tokenizer.model'\n",
    "os.environ['ckpt_dir_crmsc']   : str            =       os.environ['models_loc'] + 'llama-2-7b-chat-hf' \n",
    "os.environ['ckpt_dir_crmsc_output']   : str     =       os.environ['models_loc'] + 'llama-2-7b-chat-hf-crmsc' \n",
    "os.environ['RANK']                      =       '0'\n",
    "os.environ['WORLD_SIZE']                =       '1'\n",
    "os.environ['MASTER_ADDR']               =       'localhost'\n",
    "os.environ['MASTER_PORT']               =       '12355'\n",
    "B_INST, E_INST              =   \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS                =   \"<<SYS>>\", \"<</SYS>>\"\n",
    "PAD                         =   '[PAD]'\n",
    "\n",
    "# Use when we are going to ignore splitting\n",
    "train_dataset               =       [\n",
    "    'env_1 - converted.txt',    \n",
    "    'env_2 - converted.txt',\n",
    "        'eenv_1 - converted.txt',\n",
    "    # 'env_3 - converted.txt',\n",
    "    # 'env_general - converted.txt',\n",
    "    ]\n",
    "validation_dataset               =       [\n",
    "    'eenv_1 - converted.txt',\n",
    "    # 'eenv_1 - converted.txt',\n",
    "    # 'eenv_2 - converted.txt',\n",
    "    ]\n",
    "\n",
    "bnb_4bit_compute_dtype                  =       'float16' # Compute dtype for 4-bit base models\n",
    "use_4bit                                =       True # Activate 4-bit precision base model loading\n",
    "bnb_4bit_quant_type                     =       'nf4' # Quantization type (fp4 or nf4)\n",
    "use_nested_quant                        =       False # Activate nested quantization for 4-bit base models\n",
    "__cuda                                  =       torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "device_map                              =       __cuda#{\"\": 0 } # Load the entire \n",
    "lora_alpha                              =       64 \n",
    "lora_dropout                            =       0.05\n",
    "lora_r                                  =       16 # might be too much, needs to be modified later\n",
    "per_device_train_batch_size     =   3\n",
    "per_device_eval_batch_size      =   3\n",
    "gradient_accumulation_steps     =   1       #  Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "eval_accumulation_steps         =   1       #  Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU.\n",
    "eval_delay                      =   0      #  Number of epochs or steps to wait for before the first evaluation can be performed\n",
    "gradient_checkpointing          =   False   # Default is false,  If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "num_train_epochs                =   15\n",
    "# Optimizer\n",
    "optim                           =   training_args.OptimizerNames('paged_adamw_32bit')\n",
    "\n",
    "logging_steps                   =   1   # log every x updates steps\n",
    "learning_rate                   =   5e-4\n",
    "fp16                            =   True   #   Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
    "bf16                            =   False   #   Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training\n",
    "max_grad_norm                   =   1e-4     #   Maximum gradient norm (for gradient clipping). default is 1.0\n",
    "# max_steps                       =   200      #   number of optimizer update steps / training steps to perform\n",
    "warmup_ratio                    =   0.0     #   Ratio of total training steps used for a linear warmup from 0 to learning_rate.\n",
    "warmup_steps                    =   110\n",
    "weight_decay                    =   0.0\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "lr_scheduler_type               =   SchedulerType.LINEAR\n",
    "\n",
    "group_by_length                 =   False   # Default is false\n",
    "max_seq_length                  =   512\n",
    "packing                         =   False # use packing dataset training\n",
    "evalaution_strategy             =   IntervalStrategy.STEPS\n",
    "eval_steps                      =   1\n",
    "saving_strategy                 =   IntervalStrategy.STEPS\n",
    "save_steps                      =   5  # save every x steps\n",
    "\n",
    "load_best_model_at_end          =   True\n",
    "metric_for_best_model           =   'eval_loss'\n",
    "greater_is_better               =   False\n",
    "# settings for tokenizer\n",
    "padding_side                    =   'right'\n",
    "max_length                      =   max_seq_length # this might be as same as max_seq_length, but for making a difference between trainer and tokenizer, we defined this parameter\n",
    "clean_up_tokenization_spaces    =   True # False by default\n",
    "use_default_system_prompt       =   True # False by default\n",
    "# Inhertir from Guardrail ML ( https://colab.research.google.com/drive/134o_cXcMe_lsvl15ZE_4Y75Kstepsntu?usp=sharing#scrollTo=nAMzy_0FtaUZ )\n",
    "def load_model( padding_side : str = padding_side):\n",
    "    compute_dtype   =   getattr(torch,bnb_4bit_compute_dtype) # focusing on 4 bits quantization\n",
    "    print( f\"Compute dtype is < {compute_dtype} >\")\n",
    "    bnb_config      =   BitsAndBytesConfig (\n",
    "        load_in_4bit    =   use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant\n",
    "    )\n",
    "    \n",
    "    if compute_dtype == torch.float16 and use_4bit:\n",
    "        major , _ = torch.cuda.get_device_capability()\n",
    "        if major >= 8:\n",
    "            print( \"*\" * 20 , \"Your GPU suports bfloat16, accelerating with the argument --bfl6\" , \"=\" * 20 )\n",
    "    \n",
    "    # Initializing the model\n",
    "    model   =   AutoModelForCausalLM.from_pretrained( os.environ['ckpt_dir_crmsc'],\n",
    "                                                    device_map              =   __cuda,\n",
    "                                                    quantization_config     =   bnb_config,\n",
    "                                                      )\n",
    "    model.config.use_cache      =   False   # Whether or not the model should return the last key/values attentions\n",
    "    model.config.pretraining_tp =   1       # for faster computation, but inaccurate, increase for better accuracy but slow calculation\n",
    "\n",
    "    # Initializing Parameter-Efficient Fine-Tuning configuration (Peft)\n",
    "    # Harnessing Low-Rank approximation technique\n",
    "    peft_config     =   LoraConfig(\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        r=lora_r,\n",
    "        # target_modules=[\n",
    "        # \"q_proj\",\n",
    "        # \"k_proj\",\n",
    "        # \"v_proj\",\n",
    "        # \"o_proj\",\n",
    "        # \"gate_proj\",\n",
    "        # \"up_proj\",\n",
    "        # \"down_proj\",\n",
    "        # \"lm_head\",\n",
    "        # ],\n",
    "        bias='none',\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "\n",
    "    # Finally, loading tokenizer\n",
    "    # we use models location instead of '.model' to avoid warning, as in new version (v5) will be deprecated, also trust argument needs to be checked later\n",
    "    tokenizer       =   LlamaTokenizer.from_pretrained( os.environ['ckpt_dir_crmsc'] , \n",
    "                                                      trust_remote_code=True,\n",
    "                                                    \n",
    "                                                      padding_side=padding_side,\n",
    "                                                      add_bos_token=False,   # bos is True by default\n",
    "                                                      add_eos_token=False,   # eos is False by default\n",
    "                                                      clean_up_tokenization_spaces  =   clean_up_tokenization_spaces, \n",
    "                                                      use_default_system_prompt     =   use_default_system_prompt,\n",
    "                                                      ) \n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # or 'tokenizer.eos_token\n",
    "    tokenizer.pad_token = E_INST\n",
    "    tokenizer.return_special_tokens_mask    =   True # Because of DataCollator request in help documentation\n",
    "    tokenizer.special_tokens_mask           =   \"[MASK]\"\n",
    "    return model , tokenizer , peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model , tokenizer , peft_config     =   load_model()\n",
    "peftmodel   =   get_peft_model(model,peft_config)\n",
    "print( peft_config )\n",
    "print( peftmodel.print_trainable_parameters() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( peftmodel )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Optimizer and LrScheduler\n",
    "# optimizer           =   AdamW( peftmodel.parameters() , lr = learning_rate )\n",
    "# lr_scheduler        =   get_scheduler( SchedulerType.CONSTANT_WITH_WARMUP , optimizer , num_warmup_steps=warmup_steps )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our dataset has already been formatted to a well-shaped format, need to include padding.\n",
    "def tokenize( sample ):\n",
    "    # return tokenizer( sample['text'] , max_length=max_length , padding='max_length' )\n",
    "    return tokenizer( sample['text']  )\n",
    "# Finding some general information about dataset(s)\n",
    "dataset_builder_env         =       load_dataset_builder( \"kiarash13p/crmsc-envs\")\n",
    "# dataset_env                 =       load_dataset(\"kiarash13p/crmsc-envs\" , data_files={ 'train': train_dataset , 'validation' : validation_dataset}).shuffle(seed=42).flatten_indices()\n",
    "# do not forget to put all dicts in '[]' on huggingface if its json\n",
    "# Flattening makes the reading faster ( 10x according to website guides )\n",
    "dataset_env                 =       load_dataset(\"kiarash13p/crmsc-envs\" , split='train')\n",
    "print( f\"Description: {dataset_builder_env.info.description}\" )\n",
    "print( f\"Features: {dataset_builder_env.info.features}\" )\n",
    "\n",
    "#shuffling and selecting -> later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_env_split  = dataset_env.train_test_split( train_size=0.7 , shuffle=True , seed=list(numpy.random.randint(10,399,len(dataset_env))))\n",
    "dataset_env_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(dataset_env_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_env       =   dataset_env_split.map( tokenize )\n",
    "# lengths                     =   [len(x['input_ids']) for x in list(tokenized_dataset_env['train']) + list(tokenized_dataset_env['test'])]\n",
    "lengths_train , lengths_validation                     =   [len(x['input_ids']) for x in list(tokenized_dataset_env['train'])] ,[len(x['input_ids']) for x in list(tokenized_dataset_env['test'])]\n",
    "print( f\"Number of tokenized strings on train dataset is : {len(lengths_train)}\")\n",
    "print( f\"Number of tokenized strings on validation dataset is : {len(lengths_validation)}\")\n",
    "fig , (ax1)= plt.subplots( 1 , 1, sharex = False , sharey = False , figsize = (18 , 6) )\n",
    "ax1.hist( lengths_train , bins=50, color='green' , alpha = 0.5 , label='train')\n",
    "ax1.hist( lengths_validation , bins=50, color='red' , alpha = 0.2 , label='validation')\n",
    "ax1.set_xlabel('Length of input_ids')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of lengths of tokens')\n",
    "ax1.legend( loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_name   =   \"epoch 15 - alpha 64 - warmup 110 - r 16 - splitrandom 0.7 - seed array random numpy - dropout 0.05\"\n",
    "training_args   =   TrainingArguments(\n",
    "    output_dir=os.environ['ckpt_dir_crmsc_output'],\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=evalaution_strategy,\n",
    "    eval_steps=eval_steps,\n",
    "    # prediction_loss_only=True, # useless feature\n",
    "    logging_dir=os.environ['ckpt_dir_crmsc_output']+'/runs/'+f\"CRMSC -- {datetime.datetime.now().strftime('%Y-%m-%d %H %M')} -- {specific_name}\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    # auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    eval_accumulation_steps=eval_accumulation_steps,\n",
    "    eval_delay=eval_delay,\n",
    "    weight_decay=weight_decay,\n",
    "    optim=optim,\n",
    "    save_strategy=saving_strategy,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    # max_steps=max_steps,\n",
    "    # warmup_ratio=warmup_ratio,\n",
    "    warmup_steps=warmup_steps,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=['tensorboard'],\n",
    "    save_total_limit=4,\n",
    "    disable_tqdm=True,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    "    metric_for_best_model=metric_for_best_model,\n",
    "    save_only_model=True,\n",
    "    greater_is_better=greater_is_better,\n",
    "    run_name= f\"CRMSC -- {datetime.datetime.now().strftime('%Y-%m-%d %H %M')}\"\n",
    ")\n",
    "\n",
    "trainer     =   SFTTrainer(\n",
    "    model=peftmodel, # this is tricky, and we need to change to model later ( there is an issue with CUDA and seems bug come from transformers or dataset cannot be uploaded to cuda)\n",
    "    train_dataset=dataset_env_split['train'],\n",
    "    eval_dataset=dataset_env_split['test'], # Validation is the correct one but we use this key \n",
    "    # peft_config=peft_config,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=max_seq_length, # Default is 1024 for COnstantLengthDataset, but we set it as None\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=packing,\n",
    "    data_collator=DataCollatorForLanguageModeling( tokenizer=tokenizer , mlm=False, return_tensors='pt')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.train() # print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( results )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(os.environ['ckpt_dir_crmsc_output'])\n",
    "peftmodel.save_pretrained(os.environ['ckpt_dir_crmsc_output'] + \"\\\\peftmodel\")\n",
    "# trainer.tokenizer.save_pretrained(os.environ['ckpt_dir_crmsc_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del tokenizer\n",
    "# del trainer\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_max_memory_cached()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
